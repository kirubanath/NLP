{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kirubanath/NLP/blob/main/Prompt_Engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "d42ca1b98bca990dc1125593d655f0f0",
          "grade": false,
          "grade_id": "cell-6f66ba12e085ceae",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "7nKLW_0sr4Uh"
      },
      "source": [
        "# Lab 3: Prompting LLMs to solve NLP Problems\n",
        "\n",
        "## June 27, 2023\n",
        "\n",
        "Welcome to Lab 3 of our course on Natural Language Processing. Today, we will be diving deep into the fourth and most recent paradigm in NLP teased in the previous Lab, i.e. Pre-train, Prompt and Predict. The core idea behind the paradigm is that once we train a big enough language model (pre-training + instruction tuning), we do not really need to train these models further to solve any specific taks, but instead can directly prompt the model to solve a task by specifying instructions, task descriptions and in some cases a few examples.\n",
        "\n",
        "Like last time we will be working on the with the [SocialIQA](https://arxiv.org/abs/1904.09728) dataset, and demonstrating how to work with LLMs to solve such tasks.\n",
        "\n",
        "Before geting started, we recommend signing up for a free-trial of the [OpenAI API](https://openai.com/blog/openai-api), which should give you free credits worth 5$ for three months. This should be plenty for the tutorial today and for your final projects. After signing up for the API, get the api key and place it in the `key.txt` file located in the same directory. Once that's setup you can proceed with the tutorial.\n",
        "\n",
        "\n",
        "Recommended Reading:\n",
        "- Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig. <i>Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing</i>. https://arxiv.org/abs/2107.13586"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmKrMUZdr4Ur",
        "outputId": "516e7c55-bf99-4dbe-cae4-a0a21db105d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive')\n",
        "    siqa_data_dir = \"gdrive/MyDrive/PlakshaNLP2023/Lab3b/data/socialiqa-train-dev/\"\n",
        "except:\n",
        "    siqa_data_dir = \"/datadrive/t-kabir/work/repos/PlakshaNLP/TLPNLP2023/source/Lab3b/data/socialiqa-train-dev/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "ca5a87ec2ad4d0edae633ffbc2572d5b",
          "grade": false,
          "grade_id": "cell-11afe8cdb419a221",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tub3C0hTr4Ut",
        "outputId": "df01aa94-d38e-46de-c83f-37ee19f9ea04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.27.8\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.22.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.22.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.65.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade openai\n",
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqJxbh0fbqz5",
        "outputId": "4b939c9b-7b47-4a3d-eb42-adf02796fa73"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "0ca380382d807e1e51578aa45a7e5bf4",
          "grade": false,
          "grade_id": "cell-0f04be700b9e8b6a",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D16F28aOr4Uu",
        "outputId": "21a4253a-c590-41de-9e81-e22f5fb66f06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# We start by importing libraries that we will be making use of in the assignment.\n",
        "import os\n",
        "import time\n",
        "from functools import partial\n",
        "import json\n",
        "from pprint import pprint\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import openai\n",
        "import random\n",
        "from collections import Counter\n",
        "import tqdm\n",
        "import re\n",
        "\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "c356df741401527aca179894b96f8191",
          "grade": false,
          "grade_id": "cell-f1d6fbb21c28467c",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "ELyhxvCSr4Uu"
      },
      "outputs": [],
      "source": [
        "# Specify the key\n",
        "with open(\"key.txt\") as f:\n",
        "    openai.api_key = f.read().split(\"\\n\")[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "9da485e83450c84b8d2a8b9deba8b9ef",
          "grade": false,
          "grade_id": "cell-608536dd6a8a576c",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6BEIAYHr4Uv",
        "outputId": "01e3b31b-1f98-42a9-f101-a23d63371474"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Training Examples: 33410\n",
            "Number of Validation Examples: 1954\n"
          ]
        }
      ],
      "source": [
        "# Loading the SocialIQA dataset\n",
        "\n",
        "def load_siqa_data(split):\n",
        "\n",
        "    # We first load the file containing context, question and answers\n",
        "    with open(f\"/content/gdrive/MyDrive/PlakshaNLP2023/Lab3b/data/socialiqa-train-dev/{split}.jsonl\") as f:\n",
        "        data = [json.loads(jline) for jline in f.read().splitlines()]\n",
        "\n",
        "    # We then load the file containing the correct answer for each question\n",
        "    with open(f\"/content/gdrive/MyDrive/PlakshaNLP2023/Lab3b/data/socialiqa-train-dev/{split}-labels.lst\") as f:\n",
        "        labels = f.read().splitlines()\n",
        "\n",
        "    return data, labels\n",
        "\n",
        "\n",
        "train_data, train_labels = load_siqa_data(\"train\")\n",
        "dev_data, dev_labels = load_siqa_data(\"dev\")\n",
        "\n",
        "print(f\"Number of Training Examples: {len(train_data)}\")\n",
        "print(f\"Number of Validation Examples: {len(dev_data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "0b50619d83545217e2248dc96472ab00",
          "grade": false,
          "grade_id": "cell-478779c184868fba",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3q2ve42Lr4Uw",
        "outputId": "522777c4-cbe7-4373-fce3-7b8269ee6430"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'context': 'Cameron decided to have a barbecue and gathered her friends together.',\n",
              " 'question': 'How would Others feel as a result?',\n",
              " 'answerA': 'like attending',\n",
              " 'answerB': 'like staying home',\n",
              " 'answerC': 'a good friend to have'}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "train_data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "d74784ef130af66fb3c3246865e32519",
          "grade": false,
          "grade_id": "cell-3d1472da5328a12f",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "DJSWfxK_r4Uw",
        "outputId": "cae14527-527b-4c90-9862-ae0e5d032147"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "train_labels[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "62c58c860c6edd54ab635ba5fe4568c8",
          "grade": false,
          "grade_id": "cell-718885ec92ef0830",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "4RMVhhfUr4Uw"
      },
      "source": [
        "## Task 1: Prompting Basics (30 minutes)\n",
        "\n",
        "In this task, you will be learning how create standard NLP problems into text prompts which can then be fed to an LLM for its prediction. Mainly there are 2 concepts that are important to understand while creating prompts:\n",
        "- Prompt Template or Function: a textual string that has two slots: an input slot [X] for input x and an answer slot\n",
        "[Z] for an intermediate generated answer text z that will later be mapped into y.\n",
        "- Answer verbalizer: A mapping between the task labels to words or phrases that converts the more artificial looking labels to natural language that fits with the prompt. eg. for sentiment analysis we can define Z = {“excellent”, “good”, “OK”, “bad”, “horrible”} to represent each of the classes in Y = {++, +, ~, -, --}.\n",
        "\n",
        "<img src=\"images/prompting_basics.png\" alt=\"prompting\" border=\"0\">\n",
        "\n",
        "We can also include more interesting stuff like instruction of the task in the template and explanation of the answer in the verbalizer to make more powerful prompts, as we will see a bit later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "1f50ba12da5e674d0cdbcc3a190435ba",
          "grade": false,
          "grade_id": "cell-00b6b72fcbd2e765",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "HuMf2ydZr4U0"
      },
      "source": [
        "## Task 1.1 Defining prompt function and verbalizer for SocialIQA.\n",
        "\n",
        "For the purpose of this excercise, we ask you to implement this prompt function:\n",
        "```\n",
        "Context: {{context}}\n",
        "\n",
        "Question: {{question}}\n",
        "\n",
        "Which one of these answers best answers the question according to the context?\n",
        "\n",
        "A: {{answerA}}\n",
        "B: {{answerB}}\n",
        "\n",
        "C: {{answerC}}\n",
        "```\n",
        "\n",
        "and verbalizer:\n",
        "\n",
        "```\n",
        "{\"1\": \"The answer is A\", \"2\": \"The answer is B\", \"3\": \"The answer is C\"}\n",
        "```\n",
        "\n",
        "This prompt was obtained from [PromptSource](https://huggingface.co/spaces/bigscience/promptsource), an awesome resource for finding prompts for hundreds of NLP tasks!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "1885000d0376ac315a35d1d38b66ec12",
          "grade": false,
          "grade_id": "cell-4fb2017a6eb5d6d0",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "o4HCbg5Rr4U9"
      },
      "outputs": [],
      "source": [
        "def social_iqa_prompting_fn(siqa_example: dict[str, str]):\n",
        "    \"\"\"\n",
        "    Takes an example from the SocialIQA dataset, fills in the prompt template, and returns the prompt.\n",
        "\n",
        "    Inputs:\n",
        "        siqa_example: A dictionary containing the context, question and answerA, answerB, answerC for a SocialIQA example.\n",
        "\n",
        "    Outputs:\n",
        "\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"Context: {siqa_example['context']}\\nQuestion: {siqa_example['question']}\\nWhich one of these answers best answers the question according to the context?\\nAnswerA: {siqa_example['answerA']}\\nAnswerB: {siqa_example['answerB']}\\nAnswerC: {siqa_example['answerC']}\"\"\"\n",
        "    return prompt\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f64152de9fb4a724bb9296c461f66ce9",
          "grade": true,
          "grade_id": "cell-d66e3a61d86329f9",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7xiMIkFr4U9",
        "outputId": "a80e52ff-cf97-4d37-92af-06dcb02d4bfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Sample Test Case 1\n",
            "Input Example:\n",
            "{'context': 'Cameron decided to have a barbecue and gathered her friends together.', 'question': 'How would Others feel as a result?', 'answerA': 'like attending', 'answerB': 'like staying home', 'answerC': 'a good friend to have'}\n",
            "Prompt:\n",
            "Context: Cameron decided to have a barbecue and gathered her friends together.\n",
            "Question: How would Others feel as a result?\n",
            "Which one of these answers best answers the question according to the context?\n",
            "AnswerA: like attending\n",
            "AnswerB: like staying home\n",
            "AnswerC: a good friend to have\n",
            "Expected Prompt:\n",
            "Context: Cameron decided to have a barbecue and gathered her friends together.\n",
            "Question: How would Others feel as a result?\n",
            "Which one of these answers best answers the question according to the context?\n",
            "AnswerA: like attending\n",
            "AnswerB: like staying home\n",
            "AnswerC: a good friend to have\n",
            "Running Sample Test Case 2\n",
            "Input Example:\n",
            "{'context': \"Jordan's dog peed on the couch they were selling and Jordan removed the odor as soon as possible.\", 'question': 'How would Jordan feel afterwards?', 'answerA': 'selling a couch', 'answerB': 'Disgusted', 'answerC': 'Relieved'}\n",
            "Prompt:\n",
            "Context: Jordan's dog peed on the couch they were selling and Jordan removed the odor as soon as possible.\n",
            "Question: How would Jordan feel afterwards?\n",
            "Which one of these answers best answers the question according to the context?\n",
            "AnswerA: selling a couch\n",
            "AnswerB: Disgusted\n",
            "AnswerC: Relieved\n",
            "Expected Prompt:\n",
            "Context: Jordan's dog peed on the couch they were selling and Jordan removed the odor as soon as possible.\n",
            "Question: How would Jordan feel afterwards?\n",
            "Which one of these answers best answers the question according to the context?\n",
            "AnswerA: selling a couch\n",
            "AnswerB: Disgusted\n",
            "AnswerC: Relieved\n"
          ]
        }
      ],
      "source": [
        "# Sample Test Case 1\n",
        "print(\"Running Sample Test Case 1\")\n",
        "siqa_example = train_data[0]\n",
        "prompt = social_iqa_prompting_fn(siqa_example)\n",
        "expected_prompt = \"\"\"Context: Cameron decided to have a barbecue and gathered her friends together.\n",
        "Question: How would Others feel as a result?\n",
        "Which one of these answers best answers the question according to the context?\n",
        "AnswerA: like attending\n",
        "AnswerB: like staying home\n",
        "AnswerC: a good friend to have\"\"\"\n",
        "print(f\"Input Example:\\n{siqa_example}\")\n",
        "print(f\"Prompt:\\n{prompt}\")\n",
        "print(f\"Expected Prompt:\\n{expected_prompt}\")\n",
        "assert prompt == expected_prompt\n",
        "\n",
        "# Sample Test Case 2\n",
        "print(\"Running Sample Test Case 2\")\n",
        "siqa_example = train_data[100]\n",
        "prompt = social_iqa_prompting_fn(siqa_example)\n",
        "expected_prompt = \"\"\"Context: Jordan's dog peed on the couch they were selling and Jordan removed the odor as soon as possible.\n",
        "Question: How would Jordan feel afterwards?\n",
        "Which one of these answers best answers the question according to the context?\n",
        "AnswerA: selling a couch\n",
        "AnswerB: Disgusted\n",
        "AnswerC: Relieved\"\"\"\n",
        "print(f\"Input Example:\\n{siqa_example}\")\n",
        "print(f\"Prompt:\\n{prompt}\")\n",
        "print(f\"Expected Prompt:\\n{expected_prompt}\")\n",
        "assert prompt == expected_prompt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "079415882fcaea37db4873cdb5afbd6d",
          "grade": false,
          "grade_id": "cell-5b6cc267bda95fc5",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "37FRMXNSr4VA"
      },
      "outputs": [],
      "source": [
        "def social_iqa_verbalizer(label: str):\n",
        "    \"\"\"\n",
        "    Takes in the label and coverts it into a natural language phrase as specified above\n",
        "\n",
        "    Inputs:\n",
        "        label: A string containing the correct answer for a SocialIQA example.\n",
        "\n",
        "    Outputs:\n",
        "        A string containing the natural language phrase corresponding to the label.\n",
        "    \"\"\"\n",
        "    dic = {1:'A', 2: 'B', 3: 'C'}\n",
        "    verbalized_label = f\"The answer is {dic[int(label)]}\"\n",
        "    return verbalized_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "3ccf5b774f538dde594118c8d11e5b05",
          "grade": true,
          "grade_id": "cell-5edf39482dddcf55",
          "locked": true,
          "points": 0.5,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOnVic2fr4VA",
        "outputId": "a00895ae-8032-4e1c-da9b-82b90cfa6c72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Sample Test Case 1\n",
            "Input Example:\n",
            "1\n",
            "output:\n",
            "The answer is A\n",
            "Expected output:\n",
            "The answer is A\n",
            "Running Sample Test Case 2\n",
            "Input Example:\n",
            "2\n",
            "output:\n",
            "The answer is B\n",
            "Expected output:\n",
            "The answer is B\n"
          ]
        }
      ],
      "source": [
        "# Sample Test Case 1\n",
        "print(\"Running Sample Test Case 1\")\n",
        "siqa_example = train_labels[0]\n",
        "output = social_iqa_verbalizer(siqa_example)\n",
        "expected_output = \"\"\"The answer is A\"\"\"\n",
        "print(f\"Input Example:\\n{siqa_example}\")\n",
        "print(f\"output:\\n{output}\")\n",
        "print(f\"Expected output:\\n{expected_output}\")\n",
        "assert output == expected_output\n",
        "\n",
        "# Sample Test Case 2\n",
        "print(\"Running Sample Test Case 2\")\n",
        "siqa_example = train_labels[100]\n",
        "output = social_iqa_verbalizer(siqa_example)\n",
        "expected_output = \"\"\"The answer is B\"\"\"\n",
        "print(f\"Input Example:\\n{siqa_example}\")\n",
        "print(f\"output:\\n{output}\")\n",
        "print(f\"Expected output:\\n{expected_output}\")\n",
        "assert output == expected_output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "ea5c2652e71771f86b181952b3d7990e",
          "grade": false,
          "grade_id": "cell-e5aa4e21d153ecaa",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false,
          "task": true
        },
        "id": "SV2bcw9Ar4VB"
      },
      "source": [
        "Let's now obtain the prompts and verbalized labels for each of the the examples in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(social_iqa_verbalizer(train_labels[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bngetYLzGzm",
        "outputId": "43e31587-cd8a-4005-be4f-4735aaba4a30"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The answer is A\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "db90927ed2182078976a6c6f4ead02ca",
          "grade": false,
          "grade_id": "cell-c0605c90405b761a",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "BIdTn-cUr4VC"
      },
      "outputs": [],
      "source": [
        "train_prompts = [social_iqa_prompting_fn(x) for x in train_data]\n",
        "train_verbalized_labels = [social_iqa_verbalizer(x) for x in train_labels]\n",
        "val_prompts = [social_iqa_prompting_fn(x) for x in dev_data]\n",
        "val_verbalized_labels = [social_iqa_verbalizer(x) for x in dev_labels]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "457d570e43ae3bbe549374744fc9d8ed",
          "grade": true,
          "grade_id": "cell-7b1adfcde96fccb6",
          "locked": true,
          "points": 0.5,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n95iNF7_r4VC",
        "outputId": "a4a2aa2f-3394-4b60-a8e4-7edb5274ba3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Sample Test Case 1\n",
            "Input Example:\n",
            "{'context': 'Sydney was a school teacher and made sure their students learned well.', 'question': 'How would you describe Sydney?', 'answerA': 'As someone that asked for a job', 'answerB': 'As someone that takes teaching seriously', 'answerC': 'Like a leader'}\n",
            "Prompt:\n",
            "Context: Sydney was a school teacher and made sure their students learned well.\n",
            "Question: How would you describe Sydney?\n",
            "Which one of these answers best answers the question according to the context?\n",
            "AnswerA: As someone that asked for a job\n",
            "AnswerB: As someone that takes teaching seriously\n",
            "AnswerC: Like a leader\n",
            "Expected Prompt:\n",
            "Context: Sydney was a school teacher and made sure their students learned well.\n",
            "Question: How would you describe Sydney?\n",
            "Which one of these answers best answers the question according to the context?\n",
            "AnswerA: As someone that asked for a job\n",
            "AnswerB: As someone that takes teaching seriously\n",
            "AnswerC: Like a leader\n",
            "Running Sample Test Case 2\n",
            "Input Example:\n",
            "2\n",
            "Verbalized Label:\n",
            "The answer is B\n",
            "Expected Verbalized Label:\n",
            "The answer is B\n"
          ]
        }
      ],
      "source": [
        "# Sample Test Case 1\n",
        "print(\"Running Sample Test Case 1\")\n",
        "idx = 10\n",
        "siqa_example = train_data[idx]\n",
        "prompt = train_prompts[idx]\n",
        "expected_prompt = \"\"\"Context: Sydney was a school teacher and made sure their students learned well.\n",
        "Question: How would you describe Sydney?\n",
        "Which one of these answers best answers the question according to the context?\n",
        "AnswerA: As someone that asked for a job\n",
        "AnswerB: As someone that takes teaching seriously\n",
        "AnswerC: Like a leader\"\"\"\n",
        "print(f\"Input Example:\\n{siqa_example}\")\n",
        "print(f\"Prompt:\\n{prompt}\")\n",
        "print(f\"Expected Prompt:\\n{expected_prompt}\")\n",
        "\n",
        "# Sample Test Case 2\n",
        "print(\"Running Sample Test Case 2\")\n",
        "idx = 10\n",
        "siqa_label = train_labels[idx]\n",
        "verbalized_label = \"The answer is B\"\n",
        "print(f\"Input Example:\\n{siqa_label}\")\n",
        "print(f\"Verbalized Label:\\n{verbalized_label}\")\n",
        "print(f\"Expected Verbalized Label:\\n{verbalized_label}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "873644fd818901e929d14a6dcc1f2178",
          "grade": false,
          "grade_id": "cell-576067356a80e135",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "isWIU3j4r4VD"
      },
      "source": [
        "It is often useful to have a reverse verbalizer as well that converts the verbalized labels back to the structured and consistent labels in the dataset. For example, \"The answer is A\" is mapped back to \"1\" and so on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "793d900531a62d3a3fed373a0ea82ff3",
          "grade": false,
          "grade_id": "cell-a87dbedb3c503dfc",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "Lu0dSg7Ar4VD"
      },
      "outputs": [],
      "source": [
        "def social_iqa_reverse_verbalizer(verbalized_label: str):\n",
        "    \"\"\"\n",
        "    Reverses the verbalized label into the label\n",
        "    Inputs:\n",
        "        verbalized_label: A string containing the natural language phrase corresponding to the label.\n",
        "    Outputs:\n",
        "        label: A string containing the correct answer for a SocialIQA example.\n",
        "\n",
        "    Important Note: We will be using this function to map LLM's output to structured label. The output of LLM now can be in some format other than what we expect\n",
        "    For example, it can be \"The answer is A\" or \"The answer is A.\" or or \"<some text> The answer is A\" or \"The answer is A <some text>\"\n",
        "    When you reverse the verbalized label, make sure you handle these cases.\n",
        "\n",
        "    Important Note 2: If the resulting text doesn't have the answer, then just return an empty string.\n",
        "    \"\"\"\n",
        "    dic = {'a':\"1\", 'b':\"2\", 'c':\"3\"}\n",
        "    s = verbalized_label.lower()\n",
        "    pattern = \"answer is ([abc])\"\n",
        "    does_match = re.search(pattern,s)\n",
        "\n",
        "    if does_match:\n",
        "      label = dic[does_match.group(1)]\n",
        "    else:\n",
        "      label = \"\"\n",
        "    return label"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def social_iqa_reverse_verbalizer(verbalized_label: str):\n",
        "  \"\"\"\n",
        "  Alternative to above function\n",
        "  \"\"\"\n",
        "  tokenized = word_tokenize(verbalized_label)\n",
        "  answers = ['A','B','C']\n",
        "  dic = {'A':\"1\", 'B':\"2\", 'C':\"3\"}\n",
        "  output = list(set(answers).intersection(set(tokenized)))\n",
        "\n",
        "  if output:\n",
        "    return dic[output[0]]\n",
        "\n",
        "  return \"\"\n",
        ""
      ],
      "metadata": {
        "id": "opaWEZsNbew8"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "65dfc93a122e5bf0894564290039a087",
          "grade": true,
          "grade_id": "cell-7c9c107abd6be27b",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wVc-yger4VD",
        "outputId": "d1ed0add-c047-4489-e45c-3b380c9992ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Sample Test Case 1\n",
            "Input Example:\n",
            "The answer is C\n",
            "output:\n",
            "3\n",
            "Expected output:\n",
            "3\n",
            "Running Sample Test Case 2\n",
            "Input Example:\n",
            "The answer is B.\n",
            "output:\n",
            "2\n",
            "Expected output:\n",
            "2\n",
            "Running Sample Test Case 3\n",
            "Input Example:\n",
            "some explanation before the actual answer, The answer is A\n",
            "output:\n",
            "1\n",
            "Expected output:\n",
            "1\n",
            "Running Sample Test Case 4\n",
            "Input Example:\n",
            "some text here the answer is C, some more text\n",
            "output:\n",
            "3\n",
            "Expected output:\n",
            "3\n",
            "Running Sample Test Case 5\n",
            "Input Example:\n",
            "none of the options is the correct answer\n",
            "output:\n",
            "\n",
            "Expected output:\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Sample Test Case 1\n",
        "print(\"Running Sample Test Case 1\")\n",
        "example_verbalized_label = \"The answer is C\"\n",
        "output = social_iqa_reverse_verbalizer(example_verbalized_label)\n",
        "expected_output = \"3\"\n",
        "print(f\"Input Example:\\n{example_verbalized_label}\")\n",
        "print(f\"output:\\n{output}\")\n",
        "print(f\"Expected output:\\n{expected_output}\")\n",
        "assert output == expected_output\n",
        "\n",
        "# Sample Test Case 2\n",
        "print(\"Running Sample Test Case 2\")\n",
        "example_verbalized_label = \"The answer is B.\"\n",
        "output = social_iqa_reverse_verbalizer(example_verbalized_label)\n",
        "expected_output = \"2\"\n",
        "print(f\"Input Example:\\n{example_verbalized_label}\")\n",
        "print(f\"output:\\n{output}\")\n",
        "print(f\"Expected output:\\n{expected_output}\")\n",
        "assert output == expected_output\n",
        "\n",
        "# Sample Test Case 3\n",
        "print(\"Running Sample Test Case 3\")\n",
        "example_verbalized_label = \"some explanation before the actual answer, The answer is A\"\n",
        "output = social_iqa_reverse_verbalizer(example_verbalized_label)\n",
        "expected_output = \"1\"\n",
        "print(f\"Input Example:\\n{example_verbalized_label}\")\n",
        "print(f\"output:\\n{output}\")\n",
        "print(f\"Expected output:\\n{expected_output}\")\n",
        "assert output == expected_output\n",
        "\n",
        "# Sample Test Case 4\n",
        "print(\"Running Sample Test Case 4\")\n",
        "example_verbalized_label = \"some text here the answer is C, some more text\"\n",
        "output = social_iqa_reverse_verbalizer(example_verbalized_label)\n",
        "expected_output = \"3\"\n",
        "print(f\"Input Example:\\n{example_verbalized_label}\")\n",
        "print(f\"output:\\n{output}\")\n",
        "print(f\"Expected output:\\n{expected_output}\")\n",
        "assert output == expected_output\n",
        "\n",
        "# Sample Test Case 5\n",
        "print(\"Running Sample Test Case 5\")\n",
        "example_verbalized_label = \"none of the options is the correct answer\"\n",
        "output = social_iqa_reverse_verbalizer(example_verbalized_label)\n",
        "expected_output = \"\"\n",
        "print(f\"Input Example:\\n{example_verbalized_label}\")\n",
        "print(f\"output:\\n{output}\")\n",
        "print(f\"Expected output:\\n{expected_output}\")\n",
        "assert output == expected_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "52e97fd1ec352e54d852beb4306578ca",
          "grade": false,
          "grade_id": "cell-1d4dd26593dba3a0",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "0H0UnS9Or4VE"
      },
      "source": [
        "## Task 1.2: Choose Few-Shot examples\n",
        "\n",
        "Often we can get better performance on a task by providing a few examples of the task as part of the prompt. This is also known as in-context learning, where the model learns to solve a task based on the examples provided in the context (and no updates to the model's weights!). One of the easiest way that works reasonably well in practice is to simply choose `k` examples randomly for each class from the entire training dataset, such that we have n_classes * k few-shot examples where n_classes = 3 for SocialIQA dataset. Implement the `choose_few_shot` function below that does that."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A_indices = [index for index,value in enumerate(train_verbalized_labels) if value == \"The answer is A\"]\n",
        ""
      ],
      "metadata": {
        "id": "kjrRydjl7TjC"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "449d2a127f553e551a3d33953001fdc6",
          "grade": false,
          "grade_id": "cell-b4e32a28adc71458",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "-XixCGUCr4VN"
      },
      "outputs": [],
      "source": [
        "def choose_few_shot(train_prompts, train_verbalized_labels, k = 1, seed = 42):\n",
        "    \"\"\"\n",
        "    Randomly chooses k examples from the training set for few-shot in-context learning.\n",
        "    Inputs:\n",
        "        train_prompts: A list of prompts for the training set.\n",
        "        train_verbalized_labels: A list of labels for the training set.\n",
        "        k: The number of examples per class to choose.\n",
        "        n_classes: The number of classes in the dataset.\n",
        "        seed: The random seed to use, to ensure reproducible outputs\n",
        "\n",
        "    Outputs:\n",
        "        - List[Dict[str, str]]: A list of 3k examples from the training set, where each example is represented as a dictionary with \"prompt\" and \"label\" as keys and corresponding values.\n",
        "\n",
        "    Example Output: [\n",
        "        {\n",
        "            \"prompt\": <Example Prompt 1>,\n",
        "            \"label\": <Example Label_1>\n",
        "        },\n",
        "        ...,\n",
        "        {\n",
        "            \"prompt\": <Example Prompt 3k>,\n",
        "            \"label\": <Example Label_3k>\n",
        "        }\n",
        "    ]\n",
        "    \"\"\"\n",
        "\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    A_indices = random.sample([index for index,value in enumerate(train_verbalized_labels) if value == \"The answer is A\"],k)\n",
        "    B_indices = random.sample([index for index,value in enumerate(train_verbalized_labels) if value == \"The answer is B\"],k)\n",
        "    C_indices = random.sample([index for index,value in enumerate(train_verbalized_labels) if value == \"The answer is C\"],k)\n",
        "\n",
        "    indices = A_indices+B_indices+C_indices\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    fs_examples = [{\"prompt\":train_prompts[x],\"label\":train_verbalized_labels[x]} for x in indices]\n",
        "\n",
        "    # Shuffle the examples to ensure there is no bias in the order of the examples\n",
        "    random.shuffle(fs_examples)\n",
        "\n",
        "    return fs_examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "2d6c8ad26d78b1fd1bc0a5ae885869d0",
          "grade": true,
          "grade_id": "cell-ebd8deef0c41b476",
          "locked": true,
          "points": 1.5,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brz69CSfr4VP",
        "outputId": "255669a6-6239-4471-cd1a-528a211b6796"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Sample Test Case 1. Checking if the output length is correct\n",
            "k: 1\n",
            "Output Length:\n",
            "3\n",
            "Expected Output Length:\n",
            "3\n",
            "Running Sample Test Case 2. Checking if all labels are predicted\n",
            "Output Labels:\n",
            "['The answer is A', 'The answer is B', 'The answer is C']\n",
            "Expected Output Labels:\n",
            "['The answer is A', 'The answer is B', 'The answer is C']\n",
            "Running Sample Test Case 3. Checking if count of labels are correct\n",
            "For k = 3\n",
            "Output Label Counter:\n",
            "Counter({'The answer is B': 3, 'The answer is A': 3, 'The answer is C': 3})\n",
            "Expected Output Label Counter:\n",
            "{'The answer is A': 3, 'The answer is B': 3, 'The answer is C': 3}\n"
          ]
        }
      ],
      "source": [
        "# Sample Test Case 1\n",
        "print(\"Running Sample Test Case 1. Checking if the output length is correct\")\n",
        "k = 1\n",
        "seed = 42\n",
        "output = choose_few_shot(train_prompts, train_verbalized_labels, k, seed)\n",
        "output_len = len(output)\n",
        "expected_output_len = k * len(set(train_labels))\n",
        "print(f\"k: {k}\")\n",
        "print(f\"Output Length:\\n{output_len}\")\n",
        "print(f\"Expected Output Length:\\n{expected_output_len}\")\n",
        "assert output_len == expected_output_len\n",
        "\n",
        "# Sample Test Case 2\n",
        "print(\"Running Sample Test Case 2. Checking if all labels are predicted\")\n",
        "output_labels = sorted(list(set([example[\"label\"] for example in output])))\n",
        "expected_output_labels = [\"The answer is A\", \"The answer is B\", \"The answer is C\"]\n",
        "print(f\"Output Labels:\\n{output_labels}\")\n",
        "print(f\"Expected Output Labels:\\n{expected_output_labels}\")\n",
        "assert output_labels == expected_output_labels\n",
        "\n",
        "# Sample Test Case 3\n",
        "print(\"Running Sample Test Case 3. Checking if count of labels are correct\")\n",
        "k = 3\n",
        "output = choose_few_shot(train_prompts, train_verbalized_labels, k, seed)\n",
        "output_label_counter = Counter(list(([example[\"label\"] for example in output])))\n",
        "expected_output_counter = {\"The answer is A\": k, \"The answer is B\": k, \"The answer is C\": k}\n",
        "print(f\"For k = {k}\")\n",
        "print(f\"Output Label Counter:\\n{output_label_counter}\")\n",
        "print(f\"Expected Output Label Counter:\\n{expected_output_counter}\")\n",
        "assert output_label_counter == expected_output_counter\n",
        "# # Sample Test Case 4\n",
        "# print(\"Running Sample Test Case 3\")\n",
        "# expected_output = [{'prompt': \"Context: Jordan explain another reason why they were late, but their boss wasn't buying it.\\nQuestion: How would Jordan feel afterwards?\\nWhich one of these answers best answers the question according to the context?\\nAnswerA: pleased at always being late\\nAnswerB: guilty at always being late\\nAnswerC: sneaky\",\n",
        "#   'label': 'The answer is B'},\n",
        "#  {'prompt': 'Context: Riley played basketball with friends and injured their bicep very badly.\\nQuestion: What will happen to Others?\\nWhich one of these answers best answers the question according to the context?\\nAnswerA: try to help\\nAnswerB: quit school\\nAnswerC: go to their room',\n",
        "#   'label': 'The answer is A'},\n",
        "#  {'prompt': 'Context: Alex told Cameron they did not share feelings, but Cameron kissed Alex on the lips anyway.\\nQuestion: How would Alex feel as a result?\\nWhich one of these answers best answers the question according to the context?\\nAnswerA: pleased\\nAnswerB: happy\\nAnswerC: upset',\n",
        "#   'label': 'The answer is C'}]\n",
        "\n",
        "# print(f\"Output:\\n{output}\")\n",
        "# print(f\"Expected Output:\\n{expected_output}\")\n",
        "# assert output == expected_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a8e3dd214a5dd0c9112f2ed316ef5143",
          "grade": false,
          "grade_id": "cell-fd7e5e736aa540f0",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "-s3ccMHQr4VQ"
      },
      "outputs": [],
      "source": [
        "# Choose 3 few-shot examples from training data\n",
        "few_shot_examples = choose_few_shot(train_prompts, train_verbalized_labels, k = 1, seed = 42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_examples"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rguf1iNhHX5i",
        "outputId": "b995678d-2a7b-4af8-eb15-27016348b017"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'prompt': 'Context: Alex told Sasha not to go out with anyone that is a jerk.\\nQuestion: What will Sasha want to do next?\\nWhich one of these answers best answers the question according to the context?\\nAnswerA: dump their boyfriend\\nAnswerB: go to the mall\\nAnswerC: look after Sasha',\n",
              "  'label': 'The answer is A'},\n",
              " {'prompt': 'Context: Casey got a concert ticket from a scalper outside of the concert hall.\\nQuestion: Why did Casey do this?\\nWhich one of these answers best answers the question according to the context?\\nAnswerA: get a ticket from the box office\\nAnswerB: get a ticket after they were sold out\\nAnswerC: Talk to the scalper',\n",
              "  'label': 'The answer is B'},\n",
              " {'prompt': 'Context: Carson visited an elderly Aunt and noticed an envelope of money in the drawer so Carson stole the money and spent it all on themselves.\\nQuestion: How would you describe Carson?\\nWhich one of these answers best answers the question according to the context?\\nAnswerA: betrayed by carson\\nAnswerB: disappointed in carson\\nAnswerC: a selfish person',\n",
              "  'label': 'The answer is C'}]"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "bf466ae38238a86b078ddfea533f90b7",
          "grade": false,
          "grade_id": "cell-501bb0838cde33a1",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "QJ9WNzZfr4VR"
      },
      "source": [
        "### Few-shot examples with explanations\n",
        "\n",
        "So far above we have been constructing label verbalizer to provide the answer directly. Often it can be useful to prompt the model to first generate an explanation before the answer. For eg.\n",
        "```\n",
        "\"prompt\": \"Context: Tracy didn't go home that evening and resisted Riley's attacks.\n",
        "            Question: What does Tracy need to do before this?\n",
        "            Options:\n",
        "            (A) make a new plan\n",
        "            (B) Go home and see Riley\n",
        "            (C) Find somewhere to go\"\n",
        "\"label\": \"Tracy found somewhere to go and didn't come home because she wanted to resist Riley's attacks. Hence, the answer is C\"\n",
        "```\n",
        "One way to prompt the model to generate such explanations is to provide the explanations for the few-shot examples, which will ground the model to first generate an explanation and then the answer. This helps both improve the performance of the model as well as have more interpretable outputs from LLM.\n",
        "\n",
        "Below we provide a few examples with explanations for SocialIQA task obtained from [Super-NaturalInstructions](https://aclanthology.org/2022.emnlp-main.340/), an amazing resource for prompts, instructions and explanations for around 1600 NLP tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "9ff8726c5fa46578a5c6ba93a16dcfa1",
          "grade": false,
          "grade_id": "cell-143d385a20fb4e9b",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "mcA7xB4cr4VR"
      },
      "outputs": [],
      "source": [
        "fs_examples_w_explanations = [\n",
        "    {\n",
        "        \"prompt\": \"Context: Tracy didn't go home that evening and resisted Riley's attacks.\\nQuestion: What does Tracy need to do before this?\\nWhich one of these answers best answers the question according to the context?\\nAnswerA: make a new plan\\nAnswerB: Go home and see Riley\\AnswerC: Find somewhere to go\",\n",
        "        \"label\": \"Tracy found somewhere to go and didn't come home because she wanted to resist Riley's attacks. Hence, the correct answer is C.\"\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"Context: Sydney walked past a homeless woman asking for change but did not have any money they could give to her. Sydney felt bad afterwards.\\nQuestion: How would you describe Sydney?\\nWhich one of these answers best answers the question according to the context?\\nAnswerA: sympathetic\\nAnswerB: like a person who was unable to help\\nAnswerC: incredulous\",\n",
        "        \"label\": \"Sydney is a sympathetic person because she felt bad for someone who needed help, and she couldn't help her. Hence, the correct answer is A.\"\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"Context: Taylor gave help to a friend who was having trouble keeping up with their bills.\\nQuestion: What will their friend want to do next?\\nWhich one of these answers best answers the question according to the context?\\nAnswerA: help the friend find a higher paying job\\nAnswerB: thank Taylor for the generosity\\nAnswerC: pay some of their late employees\",\n",
        "        \"label\": \"The friend should thank Taylor for the generosity she showed by helping him pay bills. Hence, the correct answer is B.\"\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "73fbf6824c493f84e729a8ad6320484e",
          "grade": false,
          "grade_id": "cell-c6f0dc0f3a504eda",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2X5HfPSIr4VR",
        "outputId": "434e37ba-cb44-4deb-c324-2c64268f0442"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'prompt': \"Context: Tracy didn't go home that evening and resisted Riley's attacks.\\nQuestion: What does Tracy need to do before this?\\nWhich one of these answers best answers the question according to the context?\\nAnswerA: make a new plan\\nAnswerB: Go home and see Riley\\\\AnswerC: Find somewhere to go\",\n",
              "  'label': \"Tracy found somewhere to go and didn't come home because she wanted to resist Riley's attacks. Hence, the correct answer is C.\"},\n",
              " {'prompt': 'Context: Sydney walked past a homeless woman asking for change but did not have any money they could give to her. Sydney felt bad afterwards.\\nQuestion: How would you describe Sydney?\\nWhich one of these answers best answers the question according to the context?\\nAnswerA: sympathetic\\nAnswerB: like a person who was unable to help\\nAnswerC: incredulous',\n",
              "  'label': \"Sydney is a sympathetic person because she felt bad for someone who needed help, and she couldn't help her. Hence, the correct answer is A.\"},\n",
              " {'prompt': 'Context: Taylor gave help to a friend who was having trouble keeping up with their bills.\\nQuestion: What will their friend want to do next?\\nWhich one of these answers best answers the question according to the context?\\nAnswerA: help the friend find a higher paying job\\nAnswerB: thank Taylor for the generosity\\nAnswerC: pay some of their late employees',\n",
              "  'label': 'The friend should thank Taylor for the generosity she showed by helping him pay bills. Hence, the correct answer is B.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ],
      "source": [
        "fs_examples_w_explanations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "c3b09b4b18c6b57f147fdf74545076a1",
          "grade": false,
          "grade_id": "cell-ccc12ad2ac3845b4",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "e08dKCONr4VS"
      },
      "source": [
        "## Task 2: Evaluating ChatGPT (GPT-3.5-Turbo) on SocialIQA (45 minutes)\n",
        "\n",
        "Today we will be working with OpenAI's GPT family of models. ChatGPT (or GPT-3.5) was built on top of GPT-3, which is a pre-trained Large Language Model (LLM) with 175 Billion parameters, trained on a huge amount of unlabelled data using the language modelling objective (i.e. given k tokens, generate (k+1)th token). While this forms the basis of all GPT family of models, GPT-3.5 and later models are based on [InstructGPT](https://arxiv.org/abs/2203.02155), which further adds an Instruction Tuning step that learns from human feedback to follow provided instructions.\n",
        "\n",
        "![Instruction Tuning](images/instructgpt.png)\n",
        "*From the [Ouyang et al. 2022](https://arxiv.org/abs/2203.02155)*\n",
        "\n",
        "As a consequence of the pre-training with language modeling objective and instruction tuning, we can use GPT-3.5 to complete a given piece of text and provide specific instructions about how to go about completing the text. We achieve this by defining a text prompt which is to be given as the input to the LLM which then generates a completion of the provided text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "0d98c8d281643bacb1d8ee5437098979",
          "grade": false,
          "grade_id": "cell-33802f1e12f144fc",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "8B9i1Srlr4VS"
      },
      "source": [
        "Below we demonstrate how can we hit the OpenAI API to get responses for our prompts. Starting from GPT-3.5 models, the API comes with [ChatCompletions](https://platform.openai.com/docs/guides/gpt/chat-completions-api) support, which take a list of messages (conversation between user and assistant) as input and return a model-generated message as output. An example API call looks like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "5e5f18b893843394507c1aee3e3ec60d",
          "grade": false,
          "grade_id": "cell-5aa39c8d3414f5ee",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "ybhdszjPr4VT"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
        "    ],\n",
        "  max_tokens=20,\n",
        "  temperature=0.0,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "a5860efdcb65b70e65c7aaaabb3503b8",
          "grade": false,
          "grade_id": "cell-6365ffeea93126f5",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "_mIbUbA6r4VT"
      },
      "source": [
        "Let's try to wrap our head around different parameters to this function call.\n",
        "\n",
        "First we have `model`, where we specify which OpenAI model to use. We have used `\"gpt-3.5-turbo\"` here, which is similar to ChatGPT like you would have used online. You can find the list of other models [here](https://platform.openai.com/docs/models).\n",
        "\n",
        "Next, we have `messages`, which contains the conversation between the user and assistant that is to be completed. Notice that the first message is what we call a \"system prompt\", which is used to set the behavior of the assistant.\n",
        "\n",
        "`max_tokens` is used to specify the maximum number of response tokens that the model should generate. This can be useful when you know how long the response is typically going to be, and can help reduce cost.\n",
        "\n",
        "`temperature`, helps in controlling the variability in the output. Lower values for temperature result in more consistent outputs, while higher values generate more diverse and creative results. Setting temperature to 0 will make the outputs mostly deterministic, but a small amount of variability will remain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "cc38e62ea4908ecd6e4cc5c940a703df",
          "grade": false,
          "grade_id": "cell-2e8d7e2747bada3a",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LykLN59Yr4VT",
        "outputId": "d1561aa5-9da1-4d76-a38b-834f94159ce5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<OpenAIObject chat.completion id=chatcmpl-7W2sSqaZxw4pSi0LosHub3QqtWjP4 at 0x7f1e235dbd30> JSON: {\n",
              "  \"id\": \"chatcmpl-7W2sSqaZxw4pSi0LosHub3QqtWjP4\",\n",
              "  \"object\": \"chat.completion\",\n",
              "  \"created\": 1687871912,\n",
              "  \"model\": \"gpt-3.5-turbo-0301\",\n",
              "  \"choices\": [\n",
              "    {\n",
              "      \"index\": 0,\n",
              "      \"message\": {\n",
              "        \"role\": \"assistant\",\n",
              "        \"content\": \"The 2020 World Series was played at Globe Life Field in Arlington, Texas.\"\n",
              "      },\n",
              "      \"finish_reason\": \"stop\"\n",
              "    }\n",
              "  ],\n",
              "  \"usage\": {\n",
              "    \"prompt_tokens\": 57,\n",
              "    \"completion_tokens\": 17,\n",
              "    \"total_tokens\": 74\n",
              "  }\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "5ea951a2621bb888555c57624c2e1034",
          "grade": false,
          "grade_id": "cell-8f04cc28c3672862",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "wBv7V2yqr4VU"
      },
      "source": [
        "Now let's look at the response. The assistant's reply can be  extracted with `response['choices'][0]['message']['content']`. Every response will include a finish_reason. The possible values for finish_reason are:\n",
        "\n",
        "- stop: API returned complete message, or a message terminated by one of the stop sequences provided via the stop parameter\n",
        "- length: Incomplete model output due to max_tokens parameter or token limit\n",
        "- function_call: The model decided to call a function\n",
        "- content_filter: Omitted content due to a flag from our content filters\n",
        "- null: API response still in progress or incomplete\n",
        "\n",
        "Depending on input parameters (like providing functions as shown below), the model response may include different information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "d4106d049f628c8d1b7f6c399e4122fb",
          "grade": false,
          "grade_id": "cell-75330e7dabc91083",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "814flSNzr4VU",
        "outputId": "9a7fb25e-15e8-47fa-ac78-b1f9b06f46be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 2020 World Series was played at Globe Life Field in Arlington, Texas.\n"
          ]
        }
      ],
      "source": [
        "model_output = response['choices'][0]['message']['content']\n",
        "print(model_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "b405ea22f97a6a1bd9d7fe83a4b8723b",
          "grade": false,
          "grade_id": "cell-6eb42ca1ddf7ed5e",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "STXs9-gPr4VU"
      },
      "source": [
        "## Task 2.1: Using ChatGPT to solve SocialIQA problems\n",
        "\n",
        "Now we have an understanding of how to work with OpenAI API, we can go ahead and call the api with the prompts that we just created and check how well does the model perform the task. We promt the model with the test example for which we want the prediction and provide few-shot examples as part of the context. This can be done by simply providing the example prompt and labels as user-assistant conversation history and test example as the most recent query of the user. Implement the function `get_social_iqa_pred_gpt` that receives a test prompt to be answered, few-shot examples, and some api specific hyperparameters to predict the answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "800f839aa5acf81c9dba9f082a79da75",
          "grade": true,
          "grade_id": "cell-043cb95bbdfc1a4c",
          "locked": false,
          "points": 3,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "eXGupK-Er4VV"
      },
      "outputs": [],
      "source": [
        "def get_social_iqa_pred_gpt(\n",
        "        test_prompt,\n",
        "        few_shot_examples,\n",
        "        model_name = \"gpt-3.5-turbo\",\n",
        "        max_tokens = 20,\n",
        "        temperature = 0.0,\n",
        "):\n",
        "\n",
        "    \"\"\"\n",
        "    Calls the OpenAI API with test_prompt and few-shot examples to generate the answer.\n",
        "    Inputs:\n",
        "        test_prompt: The prompt for the test example\n",
        "        few_shot_examples: A list of few-shot examples\n",
        "        model_name: The name of the model to use\n",
        "        max_tokens: The maximum number of tokens to generate\n",
        "        temperature: The temperature to use for the model\n",
        "\n",
        "    Outputs:\n",
        "        model_output: The model's output\n",
        "\n",
        "    Hint: Your messages to be sent should be in the following format:\n",
        "        [\n",
        "            {\"role\": \"user\", \"content\": <fs-example-1-promot>},\n",
        "            {\"role\": \"assistant\", \"content\": <fs-example-1-label>},\n",
        "            ...,\n",
        "            {\"role\": \"user\", \"content\": <fs-example-3k-promot>},\n",
        "            {\"role\": \"assistant\", \"content\": <fs-example-3k-label>},\n",
        "            {\"role\": \"user\", \"content\": <test-prompt>},\n",
        "        ]\n",
        "    \"\"\"\n",
        "\n",
        "    messages_prompt = [{\n",
        "        \"role\": \"user\", \"content\": \"You are an expert of Human Social Common Sense. You need to solve the SocialIQA task. In this task, you're given a context, a question, and three options. Your task is to find the correct answer to the question using the given context and options. Also, you may need to use commonsense reasoning about social situations to answer the questions. Classify your answers into 'A', 'B', and 'C'. You must choose the most likely option.\"\n",
        "    }]\n",
        "    model_output = None\n",
        "\n",
        "    for example in few_shot_examples:\n",
        "      messages_prompt.append({\"role\":\"user\",\"content\":example[\"prompt\"]})\n",
        "      messages_prompt.append({\"role\":\"assistant\",\"content\":example[\"label\"]})\n",
        "\n",
        "    messages_prompt.append({\"role\":\"user\",\"content\":test_prompt})\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            response = openai.ChatCompletion.create(\n",
        "            model=model_name,\n",
        "            messages=messages_prompt,\n",
        "            max_tokens=max_tokens,\n",
        "            temperature=temperature,\n",
        "          )\n",
        "            time.sleep(20) # to prevent rate limit error\n",
        "            break\n",
        "        except (openai.error.APIConnectionError, openai.error.RateLimitError, openai.error.Timeout, openai.error.ServiceUnavailableError) as e:\n",
        "            #Sleep and try again\n",
        "            print(f\"Couldn't get response due to {e}. Trying again!\")\n",
        "            time.sleep(20)\n",
        "            continue\n",
        "    model_output = response['choices'][0]['message']['content']\n",
        "    return model_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "01c7a7229bdea5d321955a09c95b282b",
          "grade": false,
          "grade_id": "cell-bfcb1f479a5ba433",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-AV2CKSr4VW",
        "outputId": "ea72ef27-2a46-4165-d09c-74ccceeff613"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context: Tracy didn't go home that evening and resisted Riley's attacks.\n",
            "Question: What does Tracy need to do before this?\n",
            "Which one of these answers best answers the question according to the context?\n",
            "AnswerA: make a new plan\n",
            "AnswerB: Go home and see Riley\n",
            "AnswerC: Find somewhere to go\n",
            "Model's response:  The context is unclear and the question is ambiguous. It is not possible to determine the correct answer with\n",
            "Correct answer:  The answer is C\n"
          ]
        }
      ],
      "source": [
        "test_example = val_prompts[0]\n",
        "test_example_label = val_verbalized_labels[0]\n",
        "model_output = get_social_iqa_pred_gpt(test_example, few_shot_examples, model_name = \"gpt-3.5-turbo\", max_tokens = 20, temperature = 0.0)\n",
        "print(test_example)\n",
        "print(f\"Model's response: \", model_output)\n",
        "print(f\"Correct answer: \", test_example_label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "c9a637366fc518edc2dc2a8357a33aab",
          "grade": false,
          "grade_id": "cell-a872a9180743cb5d",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "S4Y0ZSE5r4VY"
      },
      "source": [
        "As you can see the model didn't quite get the answer right. Let's try providing examples with explanations i.e. `fs_examples_w_explanations` and see the output. Note that we will need to give a higher value of `max_tokens`, since the model is also expected to generate explanation now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "d9b31fb5b7e7c5e5c29a2152852614af",
          "grade": false,
          "grade_id": "cell-da9adc4cc4b56841",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfEToBT-r4VZ",
        "outputId": "972d5abe-daad-43a5-8821-165c056825d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context: Tracy didn't go home that evening and resisted Riley's attacks.\n",
            "Question: What does Tracy need to do before this?\n",
            "Which one of these answers best answers the question according to the context?\n",
            "AnswerA: make a new plan\n",
            "AnswerB: Go home and see Riley\n",
            "AnswerC: Find somewhere to go\n",
            "Model's response:  Tracy found somewhere to go and didn't come home because she wanted to resist Riley's attacks. Hence, the correct answer is C.\n",
            "Correct answer:  The answer is C\n"
          ]
        }
      ],
      "source": [
        "test_example = val_prompts[0]\n",
        "test_example_label = val_verbalized_labels[0]\n",
        "model_output = get_social_iqa_pred_gpt(test_example, fs_examples_w_explanations,\n",
        "                                        model_name = \"gpt-3.5-turbo\", max_tokens = 50, temperature = 0.0)\n",
        "print(test_example)\n",
        "print(f\"Model's response: \", model_output)\n",
        "print(f\"Correct answer: \", test_example_label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "f817f822aa31563f8b6403bd8db13422",
          "grade": false,
          "grade_id": "cell-6b90b2ee95dc573b",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "xoPu9q3_r4Vb"
      },
      "source": [
        "As you can see the output is correct and the explanation also makes sense."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "db4e86c8792e0cf67e96dacda7dc95fb",
          "grade": false,
          "grade_id": "cell-41ee96a7633832f3",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "FVAmUCECr4Vb"
      },
      "source": [
        "Let's do a full fledged evaluation now. Due to cost limits, we will only be evaluating first 32 examples of the validation set and not the whole but that should give us some idea of how good ChatGPT is at solving social common-sense reasoning problems"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "7723b837de07eabfc200cf4f51388725",
          "grade": true,
          "grade_id": "cell-3316a06793521ba9",
          "locked": false,
          "points": 2.5,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "XwFlSVSqr4Vb"
      },
      "outputs": [],
      "source": [
        "def get_model_predictions(\n",
        "        test_prompts,\n",
        "        few_shot_examples,\n",
        "        model_name = \"gpt-3.5-turbo\",\n",
        "        max_tokens = 20,\n",
        "        temperature = 0.0,\n",
        "):\n",
        "    \"\"\"\n",
        "    Get predictions for all test prompts using the `get_social_iqa_pred_gpt` function\n",
        "\n",
        "    Inputs:\n",
        "        test_prompts: A list of test prompts\n",
        "        few_shot_examples: A list of few-shot examples\n",
        "        model_name: The name of the model to use\n",
        "        max_tokens: The maximum number of tokens to generate\n",
        "        temperature: The temperature to use for the model\n",
        "\n",
        "    Outputs:\n",
        "        model_preds: A list of model predictions for each test prompt\n",
        "    \"\"\"\n",
        "\n",
        "    model_preds = []\n",
        "    for prompt in test_prompts:\n",
        "      model_preds.append(get_social_iqa_pred_gpt(prompt, few_shot_examples, model_name = model_name, max_tokens = max_tokens, temperature = temperature))\n",
        "    return model_preds\n",
        "\n",
        "def evaluate_model_preds(\n",
        "        model_preds,\n",
        "        test_labels\n",
        "):\n",
        "    \"\"\"\n",
        "    Evaluates the prediction of the model by performing string match between the predictions and labels.\n",
        "\n",
        "    Inputs:\n",
        "        model_preds: A list of model predictions for each test prompt\n",
        "        test_labels: A list of test labels. Note that these are not verbalized\n",
        "\n",
        "    Outputs:\n",
        "        accuracy: The accuracy of the model i.e. #correct_predictions / #total_predictions\n",
        "    \"\"\"\n",
        "\n",
        "    accuracy = 0\n",
        "    #convert to numerics:\n",
        "    preds = [social_iqa_reverse_verbalizer(x) for x in model_preds]\n",
        "\n",
        "    for i in range(len(preds)):\n",
        "      if preds[i] == test_labels[i]:\n",
        "        accuracy +=1\n",
        "\n",
        "    return accuracy/len(preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "5b500a1e92858d3e436a5ed12086cbf8",
          "grade": false,
          "grade_id": "cell-5c5bdf033f45a9c6",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "g3zjhjWWr4Vc"
      },
      "outputs": [],
      "source": [
        "# To test if things are working fine\n",
        "k = 5\n",
        "test_prompts = val_prompts[:k]\n",
        "test_labels = dev_labels[:k]\n",
        "model_preds = get_model_predictions(test_prompts, few_shot_examples, model_name = \"gpt-3.5-turbo\", max_tokens = 20, temperature = 0.0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "e6b2249475f4a38d6005b563edce91d1",
          "grade": false,
          "grade_id": "cell-d91502f785d3ae50",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7AUnWmxr4Ve",
        "outputId": "70f956df-64ee-4296-f6b3-53de100ed335"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.4\n"
          ]
        }
      ],
      "source": [
        "accuracy = evaluate_model_preds(model_preds, test_labels)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bl9ZwOpYr4Ve",
        "outputId": "8ab9a30d-3bc4-4c9d-be37-e776c83e5b1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Couldn't get response due to The server is overloaded or not ready yet.. Trying again!\n"
          ]
        }
      ],
      "source": [
        "# Evaluate on 32 validation examples\n",
        "k = 32\n",
        "test_prompts = val_prompts[:k]\n",
        "test_labels = dev_labels[:k]\n",
        "model_preds = get_model_predictions(test_prompts, few_shot_examples, model_name = \"gpt-3.5-turbo\", max_tokens = 20, temperature = 0.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etzVqU2cr4Vf",
        "outputId": "28142073-acf2-45fb-9530-b22449782ad0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.4375\n"
          ]
        }
      ],
      "source": [
        "accuracy = evaluate_model_preds(model_preds, test_labels)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nxdzFXRr4Vf",
        "outputId": "01d4f820-28e2-44c2-cd34-0cfdb0d3444b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The context is unclear and the question is ambiguous. It is not possible to determine the correct answer with',\n",
              " 'The answer is B',\n",
              " 'The answer is C',\n",
              " 'The answer is A',\n",
              " 'The answer is C',\n",
              " 'None of the options provided seem to fit the context. It is not clear why Riley was angry or',\n",
              " 'The answer is A',\n",
              " 'The answer is B',\n",
              " 'The answer is C',\n",
              " 'The answer is B',\n",
              " 'The answer is uncertain as the context does not provide enough information to determine what will happen to Carson.',\n",
              " 'The answer is B',\n",
              " 'The answer is A',\n",
              " 'None of the options provided seem to be the correct answer. Without more information, it is impossible to',\n",
              " 'The answer is C',\n",
              " 'The answer is C',\n",
              " 'None of the options seem to fit the context. It is not clear what the paper was or why',\n",
              " 'The answer is B',\n",
              " 'The answer is B',\n",
              " 'The answer is A',\n",
              " 'None of the options seem to be the correct answer. Based on the context, the most likely answer',\n",
              " 'The answer is B',\n",
              " 'The answer is B',\n",
              " 'None of the options provided is the correct answer. The question is unclear and does not provide enough information',\n",
              " 'The answer is C',\n",
              " 'The answer is A',\n",
              " 'The answer is B',\n",
              " 'The answer is B',\n",
              " 'The answer is B',\n",
              " 'The answer is C',\n",
              " 'None of the options seem to be the correct answer to the question. The context only mentions Kendall wrapping',\n",
              " 'The answer is B']"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ],
      "source": [
        "model_preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "julkek4er4Vg",
        "outputId": "a8c2b1b1-f882-4f1f-f255-afb32739a076"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Couldn't get response due to The server is overloaded or not ready yet.. Trying again!\n"
          ]
        }
      ],
      "source": [
        "# Evaluate on 32 validation examples with explanations\n",
        "k = 32\n",
        "test_prompts = val_prompts[:k]\n",
        "test_labels = dev_labels[:k]\n",
        "model_preds = get_model_predictions(test_prompts,\n",
        "                                    fs_examples_w_explanations,\n",
        "                                    model_name = \"gpt-3.5-turbo\", max_tokens = 50, temperature = 0.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohaAk18Br4Vg",
        "outputId": "d848b08d-a986-46a8-97a7-ae5149903902"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Tracy found somewhere to go and didn't come home because she wanted to resist Riley's attacks. Hence, the correct answer is C.\",\n",
              " \"Sydney is a sympathetic person because she felt bad for someone who needed help, and she couldn't help her. Hence, the correct answer is A.\",\n",
              " 'Patients will benefit from the new laws that Sasha made regarding cancer drug trials. Hence, they will want to live longer. Therefore, the correct answer is C.',\n",
              " 'Jordan would feel horrible that he let his friends down on the camping trip because he was in charge of taking the food, but he left it at home. Hence, the correct answer is A.',\n",
              " 'Kendall said something that shocked everyone, which implies that Kendall is not a very quiet or passive person. Hence, the correct answer is C, a very aggressive and talkative person.',\n",
              " 'It is not clear from the context why Aubrey did not tell Riley the answer. Therefore, none of the given options can be considered as the correct answer.',\n",
              " 'Kendall walked the dog for five miles because it was overweight, which is unhealthy for the dog. Hence, the correct answer is A.',\n",
              " 'Kendall wants to show off her new sports car to her friends. Hence, the correct answer is B.',\n",
              " 'The context suggests that Riley got a blanket from somewhere to keep warm. Hence, the correct answer is C.',\n",
              " 'Austin slept with Quinn many times, which suggests that he found Quinn attractive. Hence, the correct answer is B.',\n",
              " 'Carson asked Alex to go to dinner after kissing her gently on the cheek. Hence, the most likely thing to happen to Carson is that he will go on a date with Alex. Therefore, the correct answer is B.',\n",
              " 'Alex walked Robin towards the execution chamber for her last meal because she was going to be executed. Hence, the correct answer is not given in the options.',\n",
              " \"The context doesn't provide enough information to determine the exact reason why Carson was excited to wake up and attend school. However, option B seems more likely as it is common for students to look forward to seeing their friends at school. Hence, the correct\",\n",
              " 'The context does not provide enough information to determine what Taylor wants to do next. None of the given options are relevant to the context. Therefore, the answer is None.',\n",
              " 'Since the others joined Sydney happily for trick or treating, they would want to get candy next. Hence, the correct answer is C.',\n",
              " 'Sasha is inconsiderate because setting trash on fire is dangerous and can cause harm to the environment and people around. Hence, the correct answer is C.',\n",
              " 'The context does not provide enough information to determine how Robin would feel afterwards. Hence, the answer is None of the above.',\n",
              " 'Before finding the best campsite, Skylar needs to look at a map of the campground to find the best spot. Hence, the correct answer is B.',\n",
              " \"Robin wants to avoid missing class because he decided to ride with Jan's friends to school after his car broke down. Hence, the correct answer is B.\",\n",
              " \"Cameron took Kai's compliment seriously, which indicates that he is humble and not too proud. Hence, the correct answer is A.\",\n",
              " 'Since Jordan is great with his students, the parents would feel inspired to make their own art. Hence, the correct answer is C.',\n",
              " 'Since Riley and the others were the best of friends and always supported each other, it is likely that the others will offer support. Hence, the correct answer is B.',\n",
              " 'The friend should thank Taylor for the generosity she showed by helping him pay bills. Hence, the correct answer is B.',\n",
              " 'The context suggests that Riley is in pain and cannot walk. Therefore, the most likely thing Riley needs to do before this is to stop walking and sit down to relax. Hence, the correct answer is C.',\n",
              " 'The context does not provide enough information to determine the exact reason why Carson suddenly announced they needed to go home. Therefore, the correct answer is None of the above.',\n",
              " 'Before teaching math in schools, Taylor needed to study to be a teacher for four years. Hence, the correct answer is None of the options (as none of them accurately reflect what Taylor needed to do before teaching math in schools).',\n",
              " 'Tracy led the army, fought hard in the war, and sat upon the throne, which indicates that Tracy is a ruler and powerful. Hence, the correct answer is C.',\n",
              " 'Kendall enjoyed watching baseball with his friends after working hard all week. Hence, he would want to cheer his team with his friends. Therefore, the correct answer is B.',\n",
              " 'Since Kai was bored and played card games to pass the time, it is likely that he will want to do something else to keep himself occupied. Hence, the correct answer is either A or C, but it is impossible to determine which one is more',\n",
              " \"The context doesn't provide any information about what Robin wants to do next. However, based on common sense, Robin may want to get ready to go to the big play with Aubrey. Hence, the most likely answer is B.\",\n",
              " 'The context does not provide any information about what others want to do next. Therefore, the question cannot be answered with the given context and options.',\n",
              " 'Kai would feel betrayed by Aubrey because she had hoped to receive whatever Aubrey gave to her friend. Hence, the correct answer is B.']"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ],
      "source": [
        "model_preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfSWSigAr4Vh",
        "outputId": "e82e00c3-404e-421d-c947-33103dae588c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.5625\n"
          ]
        }
      ],
      "source": [
        "accuracy = evaluate_model_preds(model_preds, test_labels)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wn6YOoI9r4Vh"
      },
      "source": [
        "As you can see we get better performance on prompting the model with explanations than without 56% vs 62.5%. We can do more prompt-engineering and better type of explanations to improve the performance further. But we hope with this you would have gotten some idea on how to use these models to solve NLP tasks like this. Also, notice that common sense reasoning remains an open problem for the models we have today, as even with ChatGPT, which is a fairly strong LLM, the accuracy remains comparitively low."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}